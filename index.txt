1:"$Sreact.fragment"
2:I[5244,[],""]
3:I[3866,[],""]
5:I[6213,[],"OutletBoundary"]
7:I[6213,[],"MetadataBoundary"]
9:I[6213,[],"ViewportBoundary"]
b:I[4835,[],""]
:HL["/course-showcase/_next/static/media/569ce4b8f30dc480-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/course-showcase/_next/static/media/93f479601ee12b01-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/course-showcase/_next/static/css/6dfd21a10fa4025a.css","style"]
4:T518,M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z0:{"P":null,"b":"RkXEMD_fRq4lxa1SpMgg6","p":"/course-showcase","c":["",""],"i":false,"f":[[["",{"children":["__PAGE__",{}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/course-showcase/_next/static/css/6dfd21a10fa4025a.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":["$","body",null,{"className":"__variable_bb75c6 __variable_237161 antialiased","children":["$","$L2",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[],[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]]}],{"children":["__PAGE__",["$","$1","c",{"children":[["$","main",null,{"className":"bg-white min-h-screen flex flex-col items-center justify-center","children":["$","div",null,{"className":"w-full max-w-6xl px-4 mx-auto my-8","children":[["$","h1",null,{"className":"text-4xl font-bold text-blue-600 pt-10 text-center","children":"Improving Object Pose Estimation with Line Features in Mixed Reality"}],["$","p",null,{"className":"text-lg text-gray-700 mt-4 text-center","children":"ETHz Mixed Reality 2024"}],["$","div",null,{"className":"mt-6 flex flex-wrap justify-center space-x-4","children":[["$","div",null,{"className":"text-lg text-gray-700","children":"Deqing Song"}],["$","div",null,{"className":"text-lg text-gray-700","children":"Moyang Li"}],["$","div",null,{"className":"text-lg text-gray-700","children":"Yifan Jiang"}],["$","div",null,{"className":"text-lg text-gray-700","children":"Yuqiao Huang"}]]}],["$","div",null,{"className":"mt-6 flex justify-center","children":["$","a",null,{"href":"https://github.com/DavidSong2000/SBBTrainDoorLocolization","target":"_blank","className":"inline-block text-sm text-white bg-blue-600 px-4 py-2 rounded-lg hover:bg-blue-500 text-center","children":["$","span",null,{"children":[["$","svg",null,{"aria-hidden":"true","focusable":"false","data-prefix":"fab","data-icon":"github","className":"svg-inline--fa fa-github text-white text-xl text-center","role":"img","xmlns":"http://www.w3.org/2000/svg","viewBox":"0 0 496 512","style":{},"ref":"$undefined","children":["$","path",null,{"fill":"currentColor","d":"$4","style":{}}]}],"Code"]}]}]}],["$","div",null,{"className":"mt-8 w-full px-6 py-4","children":[["$","h2",null,{"className":"text-3xl font-bold text-gray-800 text-center","children":"Project Overview"}],["$","p",null,{"className":"text-lg text-gray-700 mt-4 max-w-full text-left","children":"Object pose estimation plays a critical role in augmented reality (AR) applications, particularly in environments with low-texture surfaces, such as SBB train doors. This work investigates the challenges of using line-based meth- ods for pose estimation under domain shifts, demonstrat- ing their limitations. We introduce a robust, generaliz- able approach leveraging the dense feature matcher (GIM) and a pipeline incorporating YOLOv8 bounding box pre- dictions and LIMAP-based line feature extraction. Our method enhances feature matching in domain-shifted condi- tions and provides a real-time implementation on Microsoft HoloLens, demonstrating its potential for practical AR ap- plications. Results shows the effectiveness of GIM for ro- bust feature matching and pose estimation."}]]}],["$","div",null,{"className":"mt-8 w-full px-6 py-4","children":[["$","h2",null,{"className":"text-3xl font-bold text-gray-800 text-center","children":"Pipeline"}],["$","p",null,{"className":"text-lg text-gray-700 mt-4 max-w-full text-left","children":"LiMAP is a line-based feature matching algorithm that uses line segments extracted from images to estimate object poses. The algorithm first detects line segments in the reference and query images, then matches them based on their geometric properties. By calculating the transformation matrix between the two images, LiMAP accurately localizes the object in the query image."}],["$","p",null,{"className":"text-lg text-gray-700 mt-4 max-w-full text-left","children":"GIM (Generalizable Image Matcher) is a self-supervised framework designed to learn robust image matching models with strong generalization capabilities from internet videos. By leveraging candidate correspondences between adjacent frames and propagating them to wider baselines, GIM enhances the training process for improved performance in unseen scenarios. In our project, we use GIM to localize SBB car doors, utilizing its ability to extract reliable feature correspondences."}],["$","p",null,{"className":"text-lg text-gray-700 mt-4 max-w-full text-left","children":"In this project, we compare the performance of LiMAP and GIM in feature matching accuracy, evaluating their robustness in further localization of SBB train doors."}],["$","div",null,{"className":"flex justify-center w-full","children":["$","img",null,{"src":"/course-showcase/pic/pipeline.png","alt":"LiMAP","className":"rounded-lg w-2/3 justify-center"}]}],["$","p",null,{"className":"mt-2 text-gray-500","children":"Figure 1. the top part shows the pipeline of GIM-based feature matching. GIM-based method first conducts dense feature matching between the refer- ence image and query images, then uses matched features to real- ize image warping and pose estimation. The bottom part presents the pipeline of LIMAP-based method. LIMAP-based method first extracts both line features and point features, then utilizes YOLO box to remove outliers. LIMAP-based method use filtered features to realize localization."}]]}],["$","div",null,{"className":"mt-8 w-full px-6 py-4","children":[["$","h2",null,{"className":"text-3xl font-bold text-gray-800 text-center","children":"YOLO Object Detection"}],["$","p",null,{"className":"text-lg text-gray-700 mt-4 max-w-full text-left","children":"To improve the quality of feature matching, the pipeline removes noisy line features outside the target object in the query image. This is achieved by leveraging a bounding box proposed by YOLO . To enable bounding box generation, the pipeline trains a YOLOv8 network. The YOLO pipeline structure is illustrated in Figure 2."}],["$","div",null,{"className":"flex justify-center w-full","children":["$","img",null,{"src":"/course-showcase/pic/yoloPipeline.png","alt":"YOLO","className":"w-2/3 rounded-lg"}]}],["$","p",null,{"className":"mt-2 text-gray-500","children":"Figure 2. YOLOv8 Bouding Box Pipeline. The figure illustrates the image inference process and YOLOv8 model training pipeline. The query image undergoes the preprocessing and is then fed into the trained YOLOv8 model. The training dataset of annotated images is used to fine-tune the model through a process involv- ing model training, validation and model saving. Once fine-tuned, the model processes the query image by performing tasks of pre- processing, model prediction, and post-processing to identify and bound relevant features in the image. The resulting inference high- lights SBB door areas within the query image, bounds regions around detected objects and remove the uninterested areas."}]]}],["$","div",null,{"className":"mt-8 w-full px-6 py-4","children":[["$","h2",null,{"className":"text-3xl font-bold text-gray-800 text-center","children":"LiMAP Results"}],["$","p",null,{"className":"text-lg text-gray-700 mt-4 max-w-full text-left","children":"Our experiments demonstrate that LiMAP is effective in reconstructing SBB train doors."}],["$","p",null,{"className":"text-lg text-gray-700 mt-4 max-w-full text-left","children":"Our experiments reveal that LiMAP with bounding boxes is still insufficient for accurately localizing SBB train doors with high precision. While the algorithm successfully reconstructs the line features of the SBB train doors, it struggles to transfer the learned features from the synthetic dataset to real-world query images, regardless of whether bounding boxes are applied."}],["$","h3",null,{"className":"text-2xl font-bold text-gray-800 text-center py-5","children":"SBB train door reconstruction using LiMAP"}],["$","div",null,{"className":"flex flex-wrap justify-between w-full max-w-1xl space-y-4","children":[["$","div",null,{"className":" w-full space-x-4","children":["$","video",null,{"autoPlay":true,"loop":true,"muted":true,"playsInline":true,"src":"/course-showcase/videos/web_limap_reconstruction.mp4","className":"flex-1","children":"Your browser does not support the video tag."}]}],["$","div",null,{"className":"w-full space-x-4","children":["$","video",null,{"autoPlay":true,"loop":true,"muted":true,"playsInline":true,"src":"/course-showcase/videos/web_limap_d.mp4","className":"w-full","children":"Your browser does not support the video tag."}]}]]}],["$","h3",null,{"className":"text-2xl font-bold text-gray-800 text-center py-5","children":"LiMAP 2D-3D Line Matching Results"}],["$","h4",null,{"className":"text-xl font-bold text-gray-800 text-left py-5","children":["2D-3D Line Matching ",["$","span",null,{"className":"text-red-600","children":"Without"}]," ","Bounding Box"]}],["$","div",null,{"className":"flex justify-center w-full","children":[["$","img",null,{"src":"/course-showcase/pic/out_1736.png","alt":"LiMAP","className":"w-1/3 mr-1 "}],["$","img",null,{"src":"/course-showcase/pic/out_1736_sketch.png","alt":"LiMAP","className":"w-1/3 mr-1"}],["$","img",null,{"src":"/course-showcase/pic/out_1736_3.png","alt":"LiMAP","className":"w-1/3 "}]]}],["$","h4",null,{"className":"text-xl font-bold text-gray-800 text-left py-5","children":["2D-3D Line Matching ",["$","span",null,{"className":"text-red-600","children":"Without"}]," ","Bounding Box"," ",["$","span",null,{"className":"text-yellow-600","children":"in Synthetic Image"}]]}],["$","div",null,{"className":"flex justify-center w-full","children":[["$","img",null,{"src":"/course-showcase/pic/Screenshot294.png","alt":"LiMAP","className":"w-1/3 mr-1 "}],["$","img",null,{"src":"/course-showcase/pic/294_sketch.png","alt":"LiMAP","className":"w-1/3 mr-1"}],["$","img",null,{"src":"/course-showcase/pic/294.png","alt":"LiMAP","className":"w-1/3 "}]]}],["$","h4",null,{"className":"text-xl font-bold text-gray-800 text-left py-5 mt-5","children":["2D-3D Line Matching ",["$","span",null,{"className":"text-red-600","children":"With"}]," ","Bounding Box"]}],["$","div",null,{"className":"flex justify-center w-full","children":[["$","img",null,{"src":"/course-showcase/pic/box_1736.png","alt":"LiMAP","className":"w-1/3 mr-1 "}],["$","img",null,{"src":"/course-showcase/pic/1736_sketch_box.png","alt":"LiMAP","className":"w-1/3 mr-1"}],["$","img",null,{"src":"/course-showcase/pic/1736_box.png","alt":"LiMAP","className":"w-1/3 "}]]}]]}],["$","div",null,{"className":"mt-8 w-full px-6 py-4","children":[["$","h2",null,{"className":"text-3xl font-bold text-gray-800 text-center","children":"GIM Results"}],["$","p",null,{"className":"text-lg text-gray-700 mt-4 max-w-full text-left","children":"Generalizable Image Matcher (GIM) employ internet video to enhance the robustness and generalizability in chal- lenging scenarios."}],["$","h3",null,{"className":"text-2xl font-bold text-gray-800 text-center py-5","children":"GIM Feature Matching Results"}],["$","p",null,{"className":"text-lg text-gray-700 mt-4 max-w-full text-left","children":"Our experiments show that GIM outperforms well in feature matching accuracy. The GIM algorithm is robust to occlusion and cluttered backgrounds, providing a more reliable solution for further object pose estimation in mixed reality applications."}],["$","div",null,{"className":"flex flex-wrap justify-between w-full max-w-1xl space-y-4 mt-7","children":[["$","div",null,{"className":"flex justify-between w-full space-x-4","children":[["$","video",null,{"autoPlay":true,"loop":true,"muted":true,"playsInline":true,"className":"w-1/2","src":"/course-showcase/videos/web_match1.mp4","children":"Your browser does not support the video tag."}],["$","video",null,{"autoPlay":true,"loop":true,"muted":true,"playsInline":true,"className":"w-1/2","src":"/course-showcase/videos/web_match2.mp4","children":"Your browser does not support the video tag."}]]}],["$","div",null,{"className":"flex justify-between w-full space-x-4","children":["$","video",null,{"autoPlay":true,"loop":true,"muted":true,"playsInline":true,"src":"/course-showcase/videos/web_warp1.mp4","children":"Your browser does not support the video tag."}]}],["$","div",null,{"className":"flex justify-between w-full space-x-4","children":["$","video",null,{"autoPlay":true,"loop":true,"muted":true,"playsInline":true,"src":"/course-showcase/videos/web_warp2.mp4","children":"Your browser does not support the video tag."}]}]]}]]}],["$","div",null,{"className":"mt-8 w-full px-6 py-4","children":[["$","h2",null,{"className":"text-3xl font-bold text-gray-800 text-center","children":"GIM vs LiMAP"}],["$","p",null,{"className":"text-lg text-gray-700 mt-4 max-w-full text-left","children":"We compare the performance of GIM and LiMAP, evaluate their robustness in further localization of SBB train doors."}],["$","h3",null,{"className":"text-2xl font-bold text-gray-800 text-center py-5","children":"Quantitative evaluation results"}],["$","div",null,{"className":"overflow-x-auto mt-4","children":["$","table",null,{"className":"table-auto w-full border text-black","children":[["$","caption",null,{"className":"text-lg font-bold text-gray-700 my-2","children":"Table 1. Quantitative Comparison of Localization on Synthetic Dataset"}],["$","caption",null,{"className":"text-l text-gray-500 my-2","children":"LIMAP-based method achieves better performance in terms of translation and rotation due to the introduction of line features."}],["$","thead",null,{"children":["$","tr",null,{"className":"bg-gray-200","children":[["$","th",null,{"className":"px-4 py-2 border text-black","children":"Method"}],["$","th",null,{"className":"px-4 py-2 border text-black","children":"Translation (m)"}],["$","th",null,{"className":"px-4 py-2 border text-black","children":"Rotation (deg)"}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"className":"px-4 py-2 border text-black","children":"Point-based"}],["$","td",null,{"className":"px-4 py-2 border text-black","children":"0.082"}],["$","td",null,{"className":"px-4 py-2 border text-black","children":"0.396"}]]}],["$","tr",null,{"children":[["$","td",null,{"className":"px-4 py-2 border text-black","children":"LIMAP-based [13]"}],["$","td",null,{"className":"px-4 py-2 border text-black","children":"0.032"}],["$","td",null,{"className":"px-4 py-2 border text-black","children":"0.145"}]]}]]}]]}]}],["$","div",null,{"className":"overflow-x-auto mt-4","children":["$","table",null,{"className":"table-auto w-full border text-black","children":[["$","caption",null,{"className":"text-lg font-bold text-black my-2","children":"Table 2. Comparison of Localization on Real-world Dataset"}],["$","caption",null,{"className":"text-l text-gray-500 my-2","children":"x denotes that the method fails in this sequence. GIM-based method realizes smaller absolute trajectory error (m) on 3 sequences due to its generalizability on datasets with domain shift."}],["$","thead",null,{"children":["$","tr",null,{"className":"bg-gray-200","children":[["$","th",null,{"className":"px-4 py-2 border text-black","children":"Method"}],["$","th",null,{"className":"px-4 py-2 border text-black","children":"seq1"}],["$","th",null,{"className":"px-4 py-2 border text-black","children":"seq2"}],["$","th",null,{"className":"px-4 py-2 border text-black","children":"seq3"}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"className":"px-4 py-2 border text-black","children":"LIMAP-based [13]"}],["$","td",null,{"className":"px-4 py-2 border text-black","children":"x"}],["$","td",null,{"className":"px-4 py-2 border text-black","children":"1.368 ± 0.499"}],["$","td",null,{"className":"px-4 py-2 border text-black","children":"0.958 ± 0.417"}]]}],["$","tr",null,{"children":[["$","td",null,{"className":"px-4 py-2 border text-black","children":"GIM-based [22]"}],["$","td",null,{"className":"px-4 py-2 border text-black","children":"1.005 ± 0.473"}],["$","td",null,{"className":"px-4 py-2 border text-black","children":"0.659 ± 0.378"}],["$","td",null,{"className":"px-4 py-2 border text-black","children":"0.564 ± 0.346"}]]}]]}]]}]}],["$","p",null,{"className":"text-lg text-gray-700 mt-4 max-w-full text-left","children":"Table 1 demonstrates the superiority of LIMAP-based method on low-texture images such as the SBB door. The introduction of line features improves the robustness of localization. Moreover, we conduct experiments of LIMAP-based method on real-world datasets. Table 2 show that LIMAP-based method fails on the real-world dataset due to large domain shift between the reference image and the query images. There are lots of mismatched line features even with the help of YOLO bounding box. However, the GIM-DKM model can realize more accurate feature matching and stable localization on datasets with large domain shift, as shown in Table 2."}],["$","h3",null,{"className":"text-2xl font-bold text-gray-800 text-center py-5","children":"Qualitative Comparisons of Trajectory Estimation"}],["$","div",null,{"className":"flex justify-center w-full","children":["$","img",null,{"src":"/course-showcase/pic/IMG_8188.png","alt":"GIM vs LiMAP","className":"w--full flex-1"}]}],["$","p",null,{"className":"text-lg text-gray-700 mt-4 max-w-full text-left","children":"Given the reference image from synthetic dataset and a sequence of real-world images, GIM-based method is able to realize stable feature matching and more accurate pose estimation. LIMAP-based method fails to achieve accurate point and line feature matching due to large domain shift, thus leading to the failure on all sequences"}]]}],["$","div",null,{"className":"mt-8 w-full px-6 py-4","children":[["$","h2",null,{"className":"text-3xl font-bold text-gray-800 text-center","children":"Hololens Implementation"}],["$","p",null,{"className":"text-lg text-gray-700 mt-4 max-w-full text-left","children":"We implemented the GIM algorithms on the HoloLens. By leveraging the HoloLens capabilities, we accurately localize SBB train doors in mixed reality environments, providing a reliable solution for applications in the transportation industry."}],["$","h4",null,{"className":"text-xl font-bold text-gray-800 text-left py-5","children":"Hololens real-time implementation"}],["$","div",null,{"className":"flex justify-center w-full","children":[["$","img",null,{"src":"/course-showcase/pic/holofin1.jpg","alt":"project overview","className":"w-1/3"}],["$","img",null,{"src":"/course-showcase/pic/holofin2.jpg","alt":"project overview","className":"w-1/3"}],["$","img",null,{"src":"/course-showcase/pic/holofin3.jpg","alt":"project overview","className":"w-1/3"}]]}],["$","div",null,{"className":"flex justify-center w-full mt-5","children":[["$","img",null,{"src":"/course-showcase/pic/000128_Client0_rawImage_gim_dkm_match.png","alt":"project overview","className":"w-1/2"}],["$","img",null,{"src":"/course-showcase/pic/000128_Client1_rawImage_gim_dkm_match.png","alt":"project overview","className":"w-1/2"}]]}],["$","div",null,{"className":"flex justify-center w-full mt-5","children":["$","img",null,{"src":"/course-showcase/pic/000128_Client0_rawImage_gim_dkm_warp.png","alt":"project overview","className":"flex-1"}]}],["$","div",null,{"className":"flex justify-center w-full","children":["$","img",null,{"src":"/course-showcase/pic/000128_Client1_rawImage_gim_dkm_warp.png","alt":"project overview","className":"flex-1"}]}]]}]]}]}],null,["$","$L5",null,{"children":"$L6"}]]}],{},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","NTJ1087OyWQdGP4iHzcX_",{"children":[["$","$L7",null,{"children":"$L8"}],["$","$L9",null,{"children":"$La"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}]]}],false]],"m":"$undefined","G":["$b","$undefined"],"s":false,"S":true}
a:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
8:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"Improving Object Pose Estimation with Line Features in Mixed Reality"}],["$","meta","2",{"name":"description","content":"Generated by create next app"}],["$","link","3",{"rel":"icon","href":"/course-showcase/favicon.ico","type":"image/x-icon","sizes":"16x16"}]]
6:null
